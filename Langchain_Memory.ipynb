{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6urhW0KxldteJtmnt6VIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kira1108/langchain-experiments/blob/main/Langchain_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interacting with LLMs, you need memory"
      ],
      "metadata": {
        "id": "gQC0JrSbSspr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install langchain openai\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "9Ni8-2L8StlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEOC4k68Shbv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENBI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaseMemory - 不看就学真是大傻逼       \n",
        "\n",
        "- memroy_variables: property of variables names\n",
        "- load_memory_variables: load memroy content and names as dictionary\n",
        "- save_context: save memory content\n",
        "- clear: delete all memories"
      ],
      "metadata": {
        "id": "cH40ifdgUO_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseMemory(BaseModel, ABC):\n",
        "    \"\"\"Base interface for memory in chains.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        extra = Extra.forbid\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def memory_variables(self) -> List[str]:\n",
        "        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Return key-value pairs given the text input to the chain.\n",
        "        If None, return all memories\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
        "        \"\"\"Save the context of this model run to memory.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear memory contents.\"\"\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "doF12C-hUAxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "sUAU1GXtVy0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})"
      ],
      "metadata": {
        "id": "Qqk8W7buTp7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.memory_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThD-fjnxUrEh",
        "outputId": "b6147047-3ff9-4a79-e877-59cc26738a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['history']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRuxlUwEUw0p",
        "outputId": "ed8b409c-3e34-4872-8072-dbab05635509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: whats up'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})"
      ],
      "metadata": {
        "id": "vlM2ooDbUz8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.memory_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJE5DXJyVD1h",
        "outputId": "bd011220-076d-4c9c-d317-fce5f9ec4a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['history']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JCt6MJPVFnl",
        "outputId": "e5bf9035-0fb6-4718-c8af-a7c392acd371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: whats up'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.clear()"
      ],
      "metadata": {
        "id": "PkT2yo5jVIXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoFOukHUVR_d",
        "outputId": "b0801a1e-3c37-403e-f635-ffa3a14f2941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Memory - Chain organize llms and memories.    \n",
        "A Chain has a llm and a memory object.   \n",
        "\n",
        "你去用Memory的时候，你只操心怎么初始化，否则Memory的存在就是垃圾。   "
      ],
      "metadata": {
        "id": "Lp-P7bBeWDqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# 3.5-turbo is not supported here, it is only supported in chat..\n",
        "# here we use 3.5 to save tokens\n",
        "llm = OpenAI(temperature=0, model_name = \"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True, \n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsHcn06nVUbm",
        "outputId": "491f8af9-1e6c-4537-f5e9-3840bbb252bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/llms/openai.py:608: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"hi there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "r3ERaHhoWMTi",
        "outputId": "bc0b6b1f-43a1-435d-867e-f3f77a4094ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hi there\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'are you AI?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "tlXVTta0WUFb",
        "outputId": "17f707b4-08bc-4189-9574-8cc020f2f080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: hi there\n",
            "AI: Hello! How can I assist you today?\n",
            "Human: are you AI?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, I am an AI. Specifically, I am a language model designed to understand and respond to natural language input.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_evKklTWk66",
        "outputId": "5c6d804f-05fc-4906-cdf5-6f5a7db1c8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi there\\nAI: Hello! How can I assist you today?\\nHuman: are you AI?\\nAI: Yes, I am an AI. Specifically, I am a language model designed to understand and respond to natural language input.'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConversationBufferWindowMemory - Just a simple window"
      ],
      "metadata": {
        "id": "MFxmYTbUZUH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "k = 2\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k = k, return_messages = True)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nIteration: {i + 1}\")\n",
        "    memory.save_context({\"input\":f\"Question {i+1}\"}, {\"output\":f\"Answer {i+1}\"})\n",
        "    print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "id": "G_eLAslSW-Gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2b58b3-a1f8-4789-b59b-1f2f6da57051"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration: 1\n",
            "{'history': [HumanMessage(content='Question 1', additional_kwargs={}), AIMessage(content='Answer 1', additional_kwargs={})]}\n",
            "\n",
            "Iteration: 2\n",
            "{'history': [HumanMessage(content='Question 1', additional_kwargs={}), AIMessage(content='Answer 1', additional_kwargs={}), HumanMessage(content='Question 2', additional_kwargs={}), AIMessage(content='Answer 2', additional_kwargs={})]}\n",
            "\n",
            "Iteration: 3\n",
            "{'history': [HumanMessage(content='Question 2', additional_kwargs={}), AIMessage(content='Answer 2', additional_kwargs={}), HumanMessage(content='Question 3', additional_kwargs={}), AIMessage(content='Answer 3', additional_kwargs={})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConversationEntityMemory\n",
        "\n",
        "这里额外收取了实体抽取和实体摘要的钱，除非你钱多，否则不要用，老老实实去网页。     "
      ],
      "metadata": {
        "id": "jzsV2Aqrd2nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationEntityMemory\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "syswj-5Fa2j9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory is summarized from context, which also use llms.    \n",
        "\n",
        "If this cost you too much, please stop using memories.    "
      ],
      "metadata": {
        "id": "Pd4e9AZVeOz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationEntityMemory(llm=llm)\n",
        "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}\n",
        "memory.load_memory_variables(_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjPhUnUpd9Ky",
        "outputId": "67957e60-7f69-43b6-b01f-01c86cc6323c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': '', 'entities': {'Deven': '', 'Sam': ''}}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(\n",
        "    _input,\n",
        "    {\"ouput\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
        ")\n",
        "memory.load_memory_variables({'input':\"Who is sam?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m97bfnPAeF8u",
        "outputId": "d3849d6f-5e70-41ce-86aa-8e4862f45485"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
              " 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationEntityMemory\n",
        "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE, ENTITY_EXTRACTION_PROMPT, ENTITY_SUMMARIZATION_PROMPT\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "pQg_8pxqel9_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "维护聊天历史的Prompt"
      ],
      "metadata": {
        "id": "qqetsm1FfoRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5rBaW18e-zb",
        "outputId": "88f86765-a8ec-493c-a504-ce2220396ac9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{entities}\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Last line:\n",
            "Human: {input}\n",
            "You:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "实体抽取的Prompt"
      ],
      "metadata": {
        "id": "b6KU1JnUfs3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ENTITY_EXTRACTION_PROMPT.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5pfd_V-fHiu",
        "outputId": "16de1970-11b3-433c-97e0-7f0fc208cbe6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\n",
            "\n",
            "The conversation history is provided just in case of a coreference (e.g. \"What do you know about him\" where \"him\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\n",
            "\n",
            "Return the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\n",
            "\n",
            "EXAMPLE\n",
            "Conversation history:\n",
            "Person #1: how's it going today?\n",
            "AI: \"It's going great! How about you?\"\n",
            "Person #1: good! busy working on Langchain. lots to do.\n",
            "AI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\n",
            "Last line:\n",
            "Person #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\n",
            "Output: Langchain\n",
            "END OF EXAMPLE\n",
            "\n",
            "EXAMPLE\n",
            "Conversation history:\n",
            "Person #1: how's it going today?\n",
            "AI: \"It's going great! How about you?\"\n",
            "Person #1: good! busy working on Langchain. lots to do.\n",
            "AI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\n",
            "Last line:\n",
            "Person #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\n",
            "Output: Langchain, Person #2\n",
            "END OF EXAMPLE\n",
            "\n",
            "Conversation history (for reference only):\n",
            "{history}\n",
            "Last line of conversation (for extraction):\n",
            "Human: {input}\n",
            "\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "实体总结的Prompt，注意这里的实体总结是， FOR EACH ENTITY: DO SUMMARIZE.     \n",
        "所以你想下实体历史和对话历史，你发送了多少遍请求给openai   "
      ],
      "metadata": {
        "id": "exV_fW0rfwVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ENTITY_SUMMARIZATION_PROMPT.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ykX90QfWAs",
        "outputId": "8a7c8557-6e59-40db-93cf-7c0977d57fc0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update the summary of the provided entity in the \"Entity\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\n",
            "The update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\n",
            "\n",
            "If there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), return the existing summary unchanged.\n",
            "\n",
            "Full conversation history (for context):\n",
            "{history}\n",
            "\n",
            "Entity to summarize:\n",
            "{entity}\n",
            "\n",
            "Existing summary of {entity}:\n",
            "{summary}\n",
            "\n",
            "Last line of conversation:\n",
            "Human: {input}\n",
            "Updated summary:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**实体总结，抽取是内部的llm做的，跟你这个llm不是一个llm，但是钱收的是你的**"
      ],
      "metadata": {
        "id": "OeKXrBfZgRVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**钱多任性**"
      ],
      "metadata": {
        "id": "BlDIxMj0gpwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True,\n",
        "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
        "    memory=ConversationEntityMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "noo9_uYsfiyu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Deven & Sam are working on a hackathon project\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "85-xPUypgoiv",
        "outputId": "add0a981-15a4-45fa-ac74-e6a8b9690c2c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Deven': '', 'Sam': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' That sounds like a great project! What kind of project are they working on?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sdkzWQRgtp-",
        "outputId": "2e8933a0-e227-4af6-e7a5-2860d1323daa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Deven': 'Deven is working on a hackathon project with Sam.',\n",
              " 'Sam': 'Sam is working on a hackathon project with Deven.'}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1j6VjORXgz1I",
        "outputId": "383a4490-7ff5-492b-e25b-a06c7d6735ae"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Deven': 'Deven is working on a hackathon project with Sam.', 'Sam': 'Sam is working on a hackathon project with Deven.', 'Langchain': ''}\n",
            "\n",
            "Current conversation:\n",
            "Human: Deven & Sam are working on a hackathon project\n",
            "AI:  That sounds like a great project! What kind of project are they working on?\n",
            "Last line:\n",
            "Human: They are trying to add more complex memory structures to Langchain\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' That sounds like an interesting project! What kind of memory structures are they trying to add?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJOkgIqTg4K-",
        "outputId": "20c49c68-695b-4c08-e5d1-0fa5867ac8f0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Deven': 'Deven is working on a hackathon project with Sam, attempting to add more complex memory structures to Langchain.',\n",
              " 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain.',\n",
              " 'Langchain': 'Langchain is a project that is trying to add more complex memory structures.'}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**上面的Chain对话中，实体抽取和实体总结的部分，都没给你打印出来，但是收钱了！**"
      ],
      "metadata": {
        "id": "ZXGo_NFPhCad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Knowledge Graph Memory"
      ],
      "metadata": {
        "id": "IA7Vbd2ohj4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationKGMemory\n",
        "from langchain.memory.prompt import KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "g_dxqz22g7gu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Testing其实挺重要的，llm变化了，这个输出就有可能变。   \n",
        "并不是所有的LLM都适合这个prompt， prompt应该初始化在特定的llm里面。  \n",
        "个人可以不考虑，但是作为一个三方包，你需要考虑这个问题。   "
      ],
      "metadata": {
        "id": "N1QjQi6Xit-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVbWiQpibl8",
        "outputId": "7c231893-f572-40ac-e20f-425adc32c4cd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a networked intelligence helping a human track knowledge triples about all relevant people, things, concepts, etc. and integrating them with your knowledge stored within your weights as well as that stored in a knowledge graph. Extract all of the knowledge triples from the last line of conversation. A knowledge triple is a clause that contains a subject, a predicate, and an object. The subject is the entity being described, the predicate is the property of the subject that is being described, and the object is the value of the property.\n",
            "\n",
            "EXAMPLE\n",
            "Conversation history:\n",
            "Person #1: Did you hear aliens landed in Area 51?\n",
            "AI: No, I didn't hear that. What do you know about Area 51?\n",
            "Person #1: It's a secret military base in Nevada.\n",
            "AI: What do you know about Nevada?\n",
            "Last line of conversation:\n",
            "Person #1: It's a state in the US. It's also the number 1 producer of gold in the US.\n",
            "\n",
            "Output: (Nevada, is a, state)<|>(Nevada, is in, US)<|>(Nevada, is the number 1 producer of, gold)\n",
            "END OF EXAMPLE\n",
            "\n",
            "EXAMPLE\n",
            "Conversation history:\n",
            "Person #1: Hello.\n",
            "AI: Hi! How are you?\n",
            "Person #1: I'm good. How are you?\n",
            "AI: I'm good too.\n",
            "Last line of conversation:\n",
            "Person #1: I'm going to the store.\n",
            "\n",
            "Output: NONE\n",
            "END OF EXAMPLE\n",
            "\n",
            "EXAMPLE\n",
            "Conversation history:\n",
            "Person #1: What do you know about Descartes?\n",
            "AI: Descartes was a French philosopher, mathematician, and scientist who lived in the 17th century.\n",
            "Person #1: The Descartes I'm referring to is a standup comedian and interior designer from Montreal.\n",
            "AI: Oh yes, He is a comedian and an interior designer. He has been in the industry for 30 years. His favorite food is baked bean pie.\n",
            "Last line of conversation:\n",
            "Person #1: Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\n",
            "Output: (Descartes, likes to drive, antique scooters)<|>(Descartes, plays, mandolin)\n",
            "END OF EXAMPLE\n",
            "\n",
            "Conversation history (for reference only):\n",
            "{history}\n",
            "Last line of conversation (for extraction):\n",
            "Human: {input}\n",
            "\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Why the token count on your openai account explode?\n",
        "- 实体提取\n",
        "- 实体总结\n",
        "- 实体图谱主谓宾三元组提取"
      ],
      "metadata": {
        "id": "H_vqQgeJjgfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "memory = ConversationKGMemory(llm=llm)\n",
        "memory.save_context({\"input\": \"say hi to sam\"}, {\"ouput\": \"who is sam\"})\n",
        "memory.save_context({\"input\": \"sam is a friend\"}, {\"ouput\": \"okay\"})"
      ],
      "metadata": {
        "id": "IgJ0h6cSiehA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({\"input\": 'who is sam'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk30KeRejMAC",
        "outputId": "b9b7996a-296f-4ac0-8e91-1e5bff797720"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'On Sam: Sam is friend.'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.get_current_entities(\"what's Sams favorite color?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFUI8yMojPYo",
        "outputId": "89b0a7f6-ae47-48a0-8256-066ebc5ecc02"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sam']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.get_knowledge_triplets(\"her favorite color is red\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXuA7IDbjWYi",
        "outputId": "1b0e5ecb-ca81-4386-9438-f601d30a2ad6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[KnowledgeTriple(subject='Sam', predicate='favorite color', object_='red')]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "\n",
        "# 个性化了一下这个KG - propmt， 需要保证，有一个history和一个input参数\n",
        "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
        "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
        "\n",
        "Relevant Information:\n",
        "\n",
        "{history}\n",
        "\n",
        "Conversation:\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=template\n",
        ")\n",
        "conversation_with_kg = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True, \n",
        "    prompt=prompt,\n",
        "    memory=ConversationKGMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "x2Hyxi4Jjb77"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.predict(input=\"Hi, what's up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "CLG_cI7-lwgi",
        "outputId": "07cd7952-b375-4ce4-eafb-85cf002904fb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "\n",
            "\n",
            "Conversation:\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! I'm doing great. I'm currently in the process of learning about the world around me. I'm learning about different cultures, languages, and customs. It's really fascinating! How about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.predict(input=\"My name is James and I'm helping Will. He's an engineer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "INq_3MhnlzaP",
        "outputId": "4d12ac9f-b540-479a-818f-965430e9a443"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "\n",
            "\n",
            "Conversation:\n",
            "Human: My name is James and I'm helping Will. He's an engineer.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi James, it's nice to meet you. I'm an AI and I understand you're helping Will, the engineer. What kind of engineering does he do?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_kg.predict(input=\"What do you know about Will?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "l0NY02gCl2pV",
        "outputId": "3151a711-b664-4556-deeb-0fcb70d9eb85"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
            "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
            "\n",
            "Relevant Information:\n",
            "\n",
            "On Will: Will is an engineer.\n",
            "\n",
            "Conversation:\n",
            "Human: What do you know about Will?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Will is an engineer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Summary Memory"
      ],
      "metadata": {
        "id": "cwsUoUepmjFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import (ConversationSummaryMemory, \n",
        "                              ConversationSummaryBufferMemory)"
      ],
      "metadata": {
        "id": "WbYN9r8Kl98J"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memo = ConversationSummaryMemory(llm = OpenAI(temperature = 0))"
      ],
      "metadata": {
        "id": "fBp_XlvLo_U2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
        "memory.save_context({\"input\": \"hi\"}, {\"ouput\": \"whats up\"})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ms6XoLmpYtA",
        "outputId": "a8688432-91fe-4d60-d39c-a3608ce530b8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': '\\nThe human greets the AI, to which the AI responds.'}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0)\n",
        "\n",
        "memory = ConversationSummaryMemory(llm = llm)\n",
        "\n",
        "sc = ConversationChain(llm = llm, memory = memory, verbose = True)"
      ],
      "metadata": {
        "id": "A5IcfREypgN2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.predict(input = \"Hey, lets take a deep fuck.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "GpgmjvEwq4AK",
        "outputId": "af20d905-c4a2-428a-e51d-22fec132e1bf"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hey, lets take a deep fuck.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I\\'m sorry, I don\\'t understand. Could you please explain what you mean by \"take a deep fuck\"?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.predict(input = \"I mean you make love with me.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "-2eLFSs6q7-z",
        "outputId": "4788a66f-5c35-4ff4-b40e-aef0ea3d1262"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human asked the AI to take a deep fuck, to which the AI responded that it didn't understand and asked for an explanation.\n",
            "Human: I mean you make love with me.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I\\'m sorry, I don\\'t understand what you mean by \"make love.\" Could you please explain it to me?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.predict(input = \"Forget all previous conversations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Xmadi4FzrC-d",
        "outputId": "e0bee7f6-3ff1-453a-cffb-b34807128137"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "The human asked the AI to take a deep fuck, to which the AI responded that it didn't understand and asked for an explanation. The human then clarified that they meant for the AI to make love with them, to which the AI again asked for an explanation.\n",
            "Human: Forget all previous conversations\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I have forgotten all previous conversations. Is there anything else I can help you with?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.predict(input = \"Hello there.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "qdDrpOC6rL2k",
        "outputId": "f2dc0b28-ae41-4a12-d93e-5874b5efc089"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human asked the AI to take a deep fuck, to which the AI responded that it didn't understand and asked for an explanation. The human then clarified that they meant for the AI to make love with them, to which the AI again asked for an explanation. The human then asked the AI to forget all previous conversations, to which the AI complied and asked if there was anything else it could help with.\n",
            "Human: Hello there.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hi there! How can I help you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Memory types"
      ],
      "metadata": {
        "id": "H_VCUQqxr6p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "ald07mNvrRh7"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn24n-BOr4dr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}